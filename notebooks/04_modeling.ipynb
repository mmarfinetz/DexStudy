{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Modeling\n",
    "\n",
    "This notebook trains and evaluates models for predicting DEX token market cap:\n",
    "- Baseline models (persistence, 7-day moving average)\n",
    "- ElasticNet regression\n",
    "- Random Forest regression\n",
    "\n",
    "Uses nested time series cross-validation to prevent leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from src import preprocessing as prep\n",
    "from src import evaluation as evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed panel\n",
    "panel_feat = pd.read_parquet('../data/processed/panel.parquet')\n",
    "print(f\"Loaded panel with shape: {panel_feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_baseline_predictions(y_log, dates, outer_splits=5, test_size=14):\n",
    "    \"\"\"Generate baseline predictions: persistence and 7-day moving average.\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=outer_splits, test_size=test_size)\n",
    "    preds = {'persistence': [], 'ma7': []}\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(y_log):\n",
    "        train_y = y_log.iloc[train_idx]\n",
    "        test_y = y_log.iloc[test_idx]\n",
    "        \n",
    "        # Persistence: y_t = y_{t-1}\n",
    "        pers = pd.Series(index=test_y.index, dtype=float)\n",
    "        for i in test_y.index:\n",
    "            if i - 1 in y_log.index:\n",
    "                pers[i] = y_log.loc[i - 1]\n",
    "            else:\n",
    "                pers[i] = train_y.iloc[-1]\n",
    "        \n",
    "        # 7-day MA\n",
    "        ma7 = pd.Series(index=test_y.index, dtype=float)\n",
    "        full_hist = y_log.copy()\n",
    "        for i in test_y.index:\n",
    "            window = full_hist.loc[:i-1].tail(7)\n",
    "            ma7[i] = window.mean() if len(window) > 0 else train_y.mean()\n",
    "        \n",
    "        preds['persistence'].append(pers)\n",
    "        preds['ma7'].append(ma7)\n",
    "    \n",
    "    return {k: pd.concat(v).sort_index() for k, v in preds.items()}\n",
    "\n",
    "\n",
    "def nested_cv_predictions(X, y, model_name, outer_splits=5, test_size=14):\n",
    "    \"\"\"Run nested CV producing out-of-fold predictions.\"\"\"\n",
    "    if model_name == 'ElasticNet':\n",
    "        base = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('est', ElasticNet(max_iter=10000, random_state=42))\n",
    "        ])\n",
    "        param_grid = {\n",
    "            'est__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1, 10],\n",
    "            'est__l1_ratio': [0.1, 0.5, 0.9],\n",
    "        }\n",
    "    elif model_name == 'RandomForest':\n",
    "        base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "        param_grid = {\n",
    "            'n_estimators': [200, 500],\n",
    "            'max_depth': [None, 5, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError('Unsupported model')\n",
    "    \n",
    "    outer = TimeSeriesSplit(n_splits=outer_splits, test_size=test_size)\n",
    "    oof_pred = pd.Series(index=y.index, dtype=float)\n",
    "    best_params = []\n",
    "    \n",
    "    for tr_idx, te_idx in outer.split(X):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "        \n",
    "        inner = TimeSeriesSplit(n_splits=3, test_size=7)\n",
    "        gcv = GridSearchCV(base, param_grid, cv=inner, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        gcv.fit(X_tr, y_tr)\n",
    "        \n",
    "        best = gcv.best_estimator_\n",
    "        best_params.append(gcv.best_params_)\n",
    "        oof_pred.iloc[te_idx] = best.predict(X_te)\n",
    "    \n",
    "    return oof_pred.dropna(), best_params\n",
    "\n",
    "\n",
    "def metrics_from_oof(y_log, yhat_log):\n",
    "    \"\"\"Compute metrics from out-of-fold predictions.\"\"\"\n",
    "    mask = yhat_log.index.intersection(y_log.index)\n",
    "    y_true_log = y_log.loc[mask].values\n",
    "    y_pred_log = yhat_log.loc[mask].values\n",
    "    return evals.compute_metrics(y_true_log, y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models per protocol\n",
    "protocols = sorted(panel_feat['protocol'].dropna().unique())\n",
    "all_records = []\n",
    "decision_records = []\n",
    "top_features_records = []\n",
    "\n",
    "for proto in protocols:\n",
    "    pdf = panel_feat[panel_feat['protocol'] == proto].sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Require at least 90 days with target\n",
    "    if pdf['market_cap_circulating'].notna().sum() < 90:\n",
    "        print(f\"Skipping {proto}: insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing {proto}...\")\n",
    "    \n",
    "    X_full, y_full = prep.build_feature_matrix(pdf, target_col='market_cap_circulating')\n",
    "    meta = X_full[['protocol', 'date']]\n",
    "    X = X_full.drop(columns=['protocol', 'date'])\n",
    "    y = y_full\n",
    "    \n",
    "    # Impute missing values\n",
    "    X = X.fillna(method='ffill').fillna(0.0).reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    meta = meta.reset_index(drop=True)\n",
    "    \n",
    "    # Baselines\n",
    "    base_preds = make_baseline_predictions(y, meta['date'])\n",
    "    for bname, preds in base_preds.items():\n",
    "        mets = metrics_from_oof(y, preds)\n",
    "        all_records.append({'protocol': proto, 'model': bname, **mets})\n",
    "    \n",
    "    # Learned models\n",
    "    for model_name in ['ElasticNet', 'RandomForest']:\n",
    "        yhat, best_params_list = nested_cv_predictions(X, y, model_name=model_name)\n",
    "        mets = metrics_from_oof(y, yhat)\n",
    "        all_records.append({'protocol': proto, 'model': model_name, **mets})\n",
    "        decision_records.append({\n",
    "            'protocol': proto,\n",
    "            'model': model_name,\n",
    "            'best_params_last_fold': json.dumps(best_params_list[-1] if best_params_list else {})\n",
    "        })\n",
    "        print(f\"  {model_name}: RMSE=${mets['rmse_usd']:,.0f}, R2={mets['r2']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics DataFrame\n",
    "metrics_df = pd.DataFrame(all_records)\n",
    "print(\"\\nModel performance by protocol:\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "os.makedirs('../results/tables', exist_ok=True)\n",
    "metrics_df.to_csv('../results/tables/metrics_per_protocol.csv', index=False)\n",
    "\n",
    "decision_df = pd.DataFrame(decision_records)\n",
    "decision_df.to_csv('../results/tables/decision_matrix.csv', index=False)\n",
    "\n",
    "print(\"Results saved to results/tables/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
