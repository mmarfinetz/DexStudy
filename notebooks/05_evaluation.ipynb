{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Evaluation\n",
    "\n",
    "This notebook summarizes model results and creates visualizations:\n",
    "- Overall model comparison\n",
    "- Time series plots\n",
    "- Residual analysis\n",
    "- Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "metrics_df = pd.read_csv('../results/tables/metrics_per_protocol.csv')\n",
    "print(f\"Loaded metrics for {metrics_df['protocol'].nunique()} protocols, {metrics_df['model'].nunique()} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall model comparison\n",
    "print(\"\\n=== OVERALL MODEL COMPARISON ===\")\n",
    "overall = metrics_df.groupby('model')[['rmse_usd', 'mae_usd', 'mape', 'r2', 'directional_accuracy']].mean()\n",
    "overall = overall.round(4)\n",
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save overall metrics\n",
    "overall.reset_index().to_csv('../results/tables/metrics_overall.csv', index=False)\n",
    "print(\"Overall metrics saved to results/tables/metrics_overall.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model comparison bar chart\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# R² comparison\n",
    "ax = axes[0]\n",
    "model_order = overall.sort_values('r2', ascending=False).index\n",
    "sns.barplot(data=metrics_df, x='model', y='r2', order=model_order, ax=ax)\n",
    "ax.set_title('R² Score by Model')\n",
    "ax.set_xlabel('')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# MAPE comparison\n",
    "ax = axes[1]\n",
    "sns.barplot(data=metrics_df, x='model', y='mape', order=model_order, ax=ax)\n",
    "ax.set_title('MAPE by Model')\n",
    "ax.set_xlabel('')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Directional Accuracy comparison\n",
    "ax = axes[2]\n",
    "sns.barplot(data=metrics_df, x='model', y='directional_accuracy', order=model_order, ax=ax)\n",
    "ax.set_title('Directional Accuracy by Model')\n",
    "ax.set_xlabel('')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('../results/figures', exist_ok=True)\n",
    "plt.savefig('../results/figures/model_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model performance by protocol\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "pivot_r2 = metrics_df.pivot(index='protocol', columns='model', values='r2')\n",
    "pivot_r2.plot(kind='bar', ax=ax)\n",
    "ax.set_title('R² Score by Protocol and Model')\n",
    "ax.set_xlabel('Protocol')\n",
    "ax.set_ylabel('R²')\n",
    "ax.legend(title='Model', bbox_to_anchor=(1.02, 1))\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/r2_by_protocol.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "print(f\"\\nBest model by R²: {overall['r2'].idxmax()} (R² = {overall['r2'].max():.4f})\")\n",
    "print(f\"Best model by MAPE: {overall['mape'].idxmin()} (MAPE = {overall['mape'].min():.4f})\")\n",
    "print(f\"Best model by Directional Accuracy: {overall['directional_accuracy'].idxmax()} ({overall['directional_accuracy'].max():.2%})\")\n",
    "\n",
    "# Per-protocol best models\n",
    "print(\"\\n=== BEST MODEL PER PROTOCOL ===\")\n",
    "best_per_proto = metrics_df.loc[metrics_df.groupby('protocol')['r2'].idxmax()][['protocol', 'model', 'r2', 'mape']]\n",
    "best_per_proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for top features if available\n",
    "try:\n",
    "    top_features = pd.read_csv('../results/tables/top_features.csv')\n",
    "    print(\"\\n=== TOP FEATURES ===\")\n",
    "    display(top_features.head(20))\n",
    "except FileNotFoundError:\n",
    "    print(\"Top features file not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
