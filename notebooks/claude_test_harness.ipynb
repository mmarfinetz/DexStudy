{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude Test Harness for DEX Valuation Study\n",
    "\n",
    "This notebook demonstrates an AI-powered test generation, execution, and auto-patching system using Claude.\n",
    "\n",
    "## Features\n",
    "- **Test Generation**: Claude analyzes code to generate comprehensive test suites\n",
    "- **Test Execution**: Run tests with detailed failure capture\n",
    "- **Auto-Patching**: Claude fixes failing code based on test errors\n",
    "- **Validation**: Re-run tests to confirm fixes\n",
    "\n",
    "## Workflow\n",
    "1. Analyze source code modules\n",
    "2. Generate comprehensive tests using Claude\n",
    "3. Execute tests and capture failures\n",
    "4. Generate patches for failures\n",
    "5. Apply patches and re-validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import modules to test\n",
    "import data_collection as dc\n",
    "import validation as val\n",
    "import preprocessing as prep\n",
    "import evaluation as evals\n",
    "\n",
    "# Claude interaction setup\n",
    "CLAUDE_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "# Check for Anthropic library\n",
    "try:\n",
    "    from anthropic import Anthropic\n",
    "    claude_client = Anthropic(api_key=CLAUDE_API_KEY) if CLAUDE_API_KEY else None\n",
    "except ImportError:\n",
    "    claude_client = None\n",
    "    print(\"âš ï¸ Anthropic library not installed. Using mock responses for demo.\")\n",
    "\n",
    "print(\"âœ… Environment setup complete\")\n",
    "print(f\"ðŸ“ Working directory: {os.getcwd()}\")\n",
    "print(f\"ðŸ¤– Claude API: {'Connected' if claude_client else 'Using mock mode'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Claude Test Harness Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaudeTestHarness:\n",
    "    \"\"\"AI-powered test generation, execution, and auto-patching.\"\"\"\n",
    "    \n",
    "    def __init__(self, module_path: str):\n",
    "        self.module_path = Path(module_path)\n",
    "        self.module_name = self.module_path.stem\n",
    "        self.test_results = []\n",
    "        self.patches_applied = []\n",
    "        self.generated_tests = None\n",
    "        self.claude = claude_client\n",
    "        \n",
    "    def read_source_code(self) -> str:\n",
    "        \"\"\"Read the source code from the module.\"\"\"\n",
    "        with open(self.module_path, 'r') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    def generate_tests(self, code_content: str = None) -> str:\n",
    "        \"\"\"Use Claude to generate comprehensive tests.\"\"\"\n",
    "        if code_content is None:\n",
    "            code_content = self.read_source_code()\n",
    "        \n",
    "        if self.claude:\n",
    "            prompt = f\"\"\"<role>Expert Python test engineer specializing in data validation and ML pipeline testing</role>\n",
    "\n",
    "<task>\n",
    "Analyze this module and generate comprehensive pytest tests:\n",
    "\n",
    "{code_content}\n",
    "\n",
    "Requirements:\n",
    "1. Test all functions with edge cases and boundaries\n",
    "2. Include tests for error conditions and exceptions\n",
    "3. Use pytest fixtures for test data setup\n",
    "4. Test data type handling and validation\n",
    "5. Include parametrized tests where appropriate\n",
    "6. Add clear docstrings explaining each test's purpose\n",
    "</task>\n",
    "\n",
    "<output_format>\n",
    "```python\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from {self.module_name} import *\n",
    "\n",
    "# Complete test suite here\n",
    "```\n",
    "</output_format>\"\"\"\n",
    "            \n",
    "            response = self.claude.messages.create(\n",
    "                model=\"claude-3-sonnet-20240229\",\n",
    "                max_tokens=4000,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            # Extract code from response\n",
    "            content = response.content[0].text\n",
    "            if \"```python\" in content:\n",
    "                code = content.split(\"```python\")[1].split(\"```\")[0]\n",
    "                return code.strip()\n",
    "            return content\n",
    "        else:\n",
    "            # Mock test generation for demo\n",
    "            return self._generate_mock_tests()\n",
    "    \n",
    "    def _generate_mock_tests(self) -> str:\n",
    "        \"\"\"Generate mock tests for demo purposes.\"\"\"\n",
    "        if 'validation' in str(self.module_path):\n",
    "            return \"\"\"import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from validation import cross_source_diff, validate_row\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_row():\n",
    "    return pd.Series({\n",
    "        'protocol': 'uniswap',\n",
    "        'date': '2024-01-01',\n",
    "        'tvl': 1000000,\n",
    "        'volume': 500000,\n",
    "        'fees': 1000,\n",
    "        'revenue': 500,\n",
    "        'users': 100\n",
    "    })\n",
    "\n",
    "class TestCrossSourceDiff:\n",
    "    def test_within_threshold(self):\n",
    "        assert not cross_source_diff(100, 103, threshold=0.05)\n",
    "        \n",
    "    def test_exceeds_threshold(self):\n",
    "        assert cross_source_diff(100, 110, threshold=0.05)\n",
    "        \n",
    "    def test_zero_values(self):\n",
    "        # This test will fail - intentional bug for demo\n",
    "        assert not cross_source_diff(0, 10, threshold=0.05)\n",
    "        \n",
    "    def test_negative_values(self):\n",
    "        assert not cross_source_diff(-100, -95, threshold=0.05)\n",
    "\n",
    "class TestValidateRow:\n",
    "    def test_valid_row(self, sample_row):\n",
    "        result = validate_row(sample_row)\n",
    "        assert result['qa_notes'] == ''\n",
    "        \n",
    "    def test_revenue_exceeds_fees(self, sample_row):\n",
    "        sample_row['revenue'] = 2000\n",
    "        result = validate_row(sample_row)\n",
    "        assert 'revenue_exceeds_fees' in result['qa_notes']\n",
    "        \n",
    "    def test_negative_tvl(self, sample_row):\n",
    "        sample_row['tvl'] = -1000\n",
    "        result = validate_row(sample_row)\n",
    "        assert 'negative_tvl' in result['qa_notes']\n",
    "\"\"\"\n",
    "        return \"# Mock tests for other modules\"\n",
    "    \n",
    "    def run_tests(self, test_code: str) -> Dict:\n",
    "        \"\"\"Execute tests and capture results.\"\"\"\n",
    "        # Save test code to temporary file\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='_test.py', delete=False) as f:\n",
    "            test_file = f.name\n",
    "            f.write(test_code)\n",
    "        \n",
    "        try:\n",
    "            # Run pytest\n",
    "            result = subprocess.run(\n",
    "                ['pytest', test_file, '-v', '--tb=short', '--no-header'],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            # Parse results\n",
    "            output = result.stdout + result.stderr\n",
    "            \n",
    "            # Count test results\n",
    "            passed = output.count(' PASSED')\n",
    "            failed = output.count(' FAILED')\n",
    "            errors = output.count(' ERROR')\n",
    "            \n",
    "            # Extract failure details\n",
    "            failures = []\n",
    "            if failed > 0 or errors > 0:\n",
    "                lines = output.split('\\n')\n",
    "                for i, line in enumerate(lines):\n",
    "                    if 'FAILED' in line or 'ERROR' in line:\n",
    "                        test_name = line.split('::')[-1].split(' ')[0] if '::' in line else 'unknown'\n",
    "                        # Look for error message in next lines\n",
    "                        error_msg = ''\n",
    "                        for j in range(i+1, min(i+10, len(lines))):\n",
    "                            if 'assert' in lines[j].lower() or 'error' in lines[j].lower():\n",
    "                                error_msg = lines[j].strip()\n",
    "                                break\n",
    "                        \n",
    "                        failures.append({\n",
    "                            'test_name': test_name,\n",
    "                            'error_type': 'AssertionError' if 'assert' in error_msg.lower() else 'Error',\n",
    "                            'message': error_msg or 'Test failed',\n",
    "                            'line_number': i\n",
    "                        })\n",
    "            \n",
    "            return {\n",
    "                'total': passed + failed + errors,\n",
    "                'passed': passed,\n",
    "                'failed': failed,\n",
    "                'errors': errors,\n",
    "                'failures': failures,\n",
    "                'output': output\n",
    "            }\n",
    "            \n",
    "        except subprocess.TimeoutExpired:\n",
    "            return {\n",
    "                'total': 0,\n",
    "                'passed': 0,\n",
    "                'failed': 0,\n",
    "                'errors': 1,\n",
    "                'failures': [{'test_name': 'all', 'error_type': 'Timeout', 'message': 'Test execution timed out'}],\n",
    "                'output': 'Test execution timed out after 30 seconds'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'total': 0,\n",
    "                'passed': 0,\n",
    "                'failed': 0,\n",
    "                'errors': 1,\n",
    "                'failures': [{'test_name': 'all', 'error_type': type(e).__name__, 'message': str(e)}],\n",
    "                'output': str(e)\n",
    "            }\n",
    "        finally:\n",
    "            # Clean up temp file\n",
    "            if os.path.exists(test_file):\n",
    "                os.remove(test_file)\n",
    "    \n",
    "    def generate_patch(self, failure_info: Dict, source_code: str = None, test_code: str = None) -> str:\n",
    "        \"\"\"Claude generates fix for failing code.\"\"\"\n",
    "        if source_code is None:\n",
    "            source_code = self.read_source_code()\n",
    "            \n",
    "        if self.claude:\n",
    "            prompt = f\"\"\"<role>Expert Python debugger with deep pandas/numpy knowledge</role>\n",
    "\n",
    "<task>\n",
    "Fix this failing test by modifying the source code:\n",
    "\n",
    "**Failing Test**: {failure_info['test_name']}\n",
    "**Error**: {failure_info['error_type']}: {failure_info['message']}\n",
    "\n",
    "**Test Code**:\n",
    "{test_code if test_code else 'Not provided'}\n",
    "\n",
    "**Current Implementation**:\n",
    "{source_code}\n",
    "\n",
    "Generate a minimal patch that:\n",
    "1. Fixes the specific error\n",
    "2. Maintains backward compatibility\n",
    "3. Handles edge cases properly\n",
    "4. Includes inline comments explaining the fix\n",
    "</task>\n",
    "\n",
    "<output_format>\n",
    "```python\n",
    "# Fixed implementation\n",
    "# [Complete fixed code here]\n",
    "```\n",
    "\n",
    "<explanation>\n",
    "[Why this fixes the issue]\n",
    "</explanation>\n",
    "</output_format>\"\"\"\n",
    "            \n",
    "            response = self.claude.messages.create(\n",
    "                model=\"claude-3-sonnet-20240229\",\n",
    "                max_tokens=4000,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            content = response.content[0].text\n",
    "            if \"```python\" in content:\n",
    "                code = content.split(\"```python\")[1].split(\"```\")[0]\n",
    "                return code.strip()\n",
    "            return content\n",
    "        else:\n",
    "            # Mock patch for demo\n",
    "            return self._generate_mock_patch(failure_info)\n",
    "    \n",
    "    def _generate_mock_patch(self, failure_info: Dict) -> str:\n",
    "        \"\"\"Generate mock patch for demo purposes.\"\"\"\n",
    "        if 'zero_values' in failure_info.get('test_name', ''):\n",
    "            return \"\"\"def cross_source_diff(a, b, threshold=0.05):\n",
    "    # Fixed: Handle zero values properly\n",
    "    if a == 0 and b == 0:\n",
    "        return False\n",
    "    if a == 0 or b == 0:\n",
    "        # When one value is zero, any non-zero difference exceeds threshold\n",
    "        return True\n",
    "    \n",
    "    rel_diff = abs(a - b) / max(abs(a), abs(b))\n",
    "    return rel_diff > threshold\n",
    "\n",
    "def validate_row(row):\n",
    "    issues = []\n",
    "    \n",
    "    # Check for revenue exceeding fees (illogical)\n",
    "    if pd.notna(row.get('revenue')) and pd.notna(row.get('fees')):\n",
    "        if row['revenue'] > row['fees']:\n",
    "            issues.append('revenue_exceeds_fees')\n",
    "    \n",
    "    # Check for negative values\n",
    "    for col in ['tvl', 'volume', 'users']:\n",
    "        if col in row and pd.notna(row[col]) and row[col] < 0:\n",
    "            issues.append(f'negative_{col}')\n",
    "    \n",
    "    row = row.copy()\n",
    "    row['qa_notes'] = ';'.join(issues) if issues else ''\n",
    "    return row\n",
    "\"\"\"\n",
    "        return \"# No patch needed\"\n",
    "    \n",
    "    def apply_patch(self, patch: str) -> bool:\n",
    "        \"\"\"Apply generated patch to source code.\"\"\"\n",
    "        try:\n",
    "            # Backup original file\n",
    "            backup_path = self.module_path.with_suffix('.py.backup')\n",
    "            with open(self.module_path, 'r') as f:\n",
    "                original = f.read()\n",
    "            with open(backup_path, 'w') as f:\n",
    "                f.write(original)\n",
    "            \n",
    "            # Apply patch\n",
    "            with open(self.module_path, 'w') as f:\n",
    "                f.write(patch)\n",
    "            \n",
    "            self.patches_applied.append({\n",
    "                'file': str(self.module_path),\n",
    "                'backup': str(backup_path),\n",
    "                'patch': patch[:200] + '...' if len(patch) > 200 else patch\n",
    "            })\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to apply patch: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def validate_fix(self) -> Dict:\n",
    "        \"\"\"Re-run tests to confirm fix.\"\"\"\n",
    "        if self.generated_tests:\n",
    "            return self.run_tests(self.generated_tests)\n",
    "        else:\n",
    "            return {'error': 'No tests to run'}\n",
    "    \n",
    "    def rollback_patches(self):\n",
    "        \"\"\"Rollback all applied patches.\"\"\"\n",
    "        for patch_info in reversed(self.patches_applied):\n",
    "            try:\n",
    "                with open(patch_info['backup'], 'r') as f:\n",
    "                    original = f.read()\n",
    "                with open(patch_info['file'], 'w') as f:\n",
    "                    f.write(original)\n",
    "                os.remove(patch_info['backup'])\n",
    "                print(f\"âœ… Rolled back: {patch_info['file']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to rollback {patch_info['file']}: {e}\")\n",
    "\n",
    "print(\"âœ… ClaudeTestHarness class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Test Generation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate on validation.py first (simplest module)\n",
    "harness = ClaudeTestHarness('../src/validation.py')\n",
    "\n",
    "display(Markdown(\"## ðŸ§ª Generating Tests for Validation Module\"))\n",
    "display(Markdown(\"Claude is analyzing the validation module to generate comprehensive tests...\"))\n",
    "\n",
    "# Read source code\n",
    "source_code = harness.read_source_code()\n",
    "display(Markdown(f\"**Source file**: `{harness.module_path}`\"))\n",
    "display(Markdown(f\"**Lines of code**: {len(source_code.splitlines())}\"))\n",
    "\n",
    "# Generate tests\n",
    "generated_tests = harness.generate_tests(source_code)\n",
    "harness.generated_tests = generated_tests\n",
    "\n",
    "# Display generated tests\n",
    "display(Markdown(\"### Generated Test Suite:\"))\n",
    "display(Markdown(f\"```python\\n{generated_tests[:1500]}...\\n```\"))\n",
    "\n",
    "# Count test functions\n",
    "test_count = generated_tests.count('def test_')\n",
    "display(Markdown(f\"\\n**Generated {test_count} test functions**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Test Execution & Failure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generated tests and capture failures\n",
    "display(Markdown(\"## ðŸš€ Running Generated Tests\"))\n",
    "\n",
    "results = harness.run_tests(generated_tests)\n",
    "\n",
    "# Display test results summary\n",
    "display(Markdown(f\"\"\"\n",
    "## Test Results Summary\n",
    "- **Total Tests**: {results['total']}\n",
    "- **Passed**: {results['passed']} âœ…\n",
    "- **Failed**: {results['failed']} âŒ\n",
    "- **Errors**: {results['errors']} âš ï¸\n",
    "\"\"\"))\n",
    "\n",
    "# Display failure details if any\n",
    "if results['failures']:\n",
    "    display(Markdown(\"### Failure Details:\"))\n",
    "    for i, failure in enumerate(results['failures'], 1):\n",
    "        display(Markdown(f\"\"\"\n",
    "**Failure {i}: `{failure['test_name']}`**\n",
    "- **Error Type**: {failure['error_type']}\n",
    "- **Message**: {failure['message']}\n",
    "\"\"\"))\n",
    "else:\n",
    "    display(Markdown(\"### âœ… All tests passed!\"))\n",
    "\n",
    "# Show sample of test output\n",
    "if results.get('output'):\n",
    "    display(Markdown(\"### Test Output Sample:\"))\n",
    "    output_lines = results['output'].split('\\n')[:20]\n",
    "    display(Markdown(\"```\\n\" + '\\n'.join(output_lines) + \"\\n...\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Auto-Patch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate patches for failures\n",
    "display(Markdown(\"## ðŸ”§ Auto-Generating Patches for Failures\"))\n",
    "\n",
    "patches = []\n",
    "if results['failures']:\n",
    "    display(Markdown(f\"Claude is analyzing {len(results['failures'])} failure(s) and generating fixes...\"))\n",
    "    \n",
    "    for failure in results['failures']:\n",
    "        display(Markdown(f\"\\n### Generating patch for: `{failure['test_name']}`\"))\n",
    "        \n",
    "        # Generate patch\n",
    "        patch_code = harness.generate_patch(failure, source_code, generated_tests)\n",
    "        \n",
    "        patches.append({\n",
    "            'issue': failure['test_name'],\n",
    "            'error': failure['message'],\n",
    "            'patch': patch_code\n",
    "        })\n",
    "        \n",
    "        # Display patch summary\n",
    "        display(Markdown(f\"\"\"\n",
    "**Issue**: {failure['message']}\n",
    "**Fix Applied**: Modified function to handle edge case\n",
    "\n",
    "```python\n",
    "{patch_code[:500]}...\n",
    "```\n",
    "\"\"\"))\n",
    "else:\n",
    "    display(Markdown(\"### âœ… No failures to patch!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Apply Patches & Re-run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply patches and validate fixes\n",
    "display(Markdown(\"## ðŸŽ¯ Applying Patches and Re-validating\"))\n",
    "\n",
    "if patches:\n",
    "    # Apply all generated patches\n",
    "    display(Markdown(\"### Applying Patches:\"))\n",
    "    for patch in patches:\n",
    "        success = harness.apply_patch(patch['patch'])\n",
    "        status = \"âœ…\" if success else \"âŒ\"\n",
    "        display(Markdown(f\"{status} Applied patch to fix: **{patch['issue']}**\"))\n",
    "    \n",
    "    # Re-run all tests to validate fixes\n",
    "    display(Markdown(\"\\n### Re-running Tests to Validate Fixes:\"))\n",
    "    final_results = harness.validate_fix()\n",
    "    \n",
    "    # Calculate improvement\n",
    "    improvement = final_results['passed'] - results['passed']\n",
    "    \n",
    "    display(Markdown(f\"\"\"\n",
    "## Final Validation Results\n",
    "- **All Tests Pass**: {'âœ… YES' if final_results['failed'] == 0 else 'âŒ NO'}\n",
    "- **Tests Passed**: {final_results['passed']}/{final_results['total']}\n",
    "- **Improvement**: +{improvement} tests fixed\n",
    "- **Patches Applied**: {len(patches)}\n",
    "\"\"\"))\n",
    "    \n",
    "    # Show before/after comparison\n",
    "    display(Markdown(f\"\"\"\n",
    "### Before vs After:\n",
    "| Metric | Before | After | Change |\n",
    "|--------|--------|-------|--------|\n",
    "| Passed | {results['passed']} | {final_results['passed']} | +{final_results['passed'] - results['passed']} |\n",
    "| Failed | {results['failed']} | {final_results['failed']} | {final_results['failed'] - results['failed']} |\n",
    "| Errors | {results['errors']} | {final_results['errors']} | {final_results['errors'] - results['errors']} |\n",
    "\"\"\"))\n",
    "else:\n",
    "    display(Markdown(\"### âœ… No patches needed - all tests passed!\"))\n",
    "    final_results = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Full Pipeline Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate integration tests for the full pipeline\n",
    "display(Markdown(\"## ðŸ”„ Generating Integration Tests for Full Pipeline\"))\n",
    "\n",
    "# Test all modules\n",
    "modules_to_test = [\n",
    "    ('../src/validation.py', 'Data validation functions'),\n",
    "    ('../src/preprocessing.py', 'Feature engineering with anti-leakage'),\n",
    "    ('../src/evaluation.py', 'Model evaluation metrics'),\n",
    "    ('../src/data_collection.py', 'API data collection')\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "for module_path, description in modules_to_test:\n",
    "    if Path(module_path).exists():\n",
    "        display(Markdown(f\"\\n### Testing: {Path(module_path).name}\"))\n",
    "        display(Markdown(f\"**Description**: {description}\"))\n",
    "        \n",
    "        # Create harness for module\n",
    "        module_harness = ClaudeTestHarness(module_path)\n",
    "        \n",
    "        # Generate and run tests\n",
    "        tests = module_harness.generate_tests()\n",
    "        results = module_harness.run_tests(tests)\n",
    "        \n",
    "        all_results[Path(module_path).stem] = results\n",
    "        \n",
    "        # Display mini summary\n",
    "        status_icon = \"âœ…\" if results['failed'] == 0 else \"âš ï¸\"\n",
    "        display(Markdown(f\"{status_icon} **{results['passed']}/{results['total']}** tests passed\"))\n",
    "\n",
    "# Overall summary\n",
    "display(Markdown(\"\\n## ðŸ“Š Overall Test Coverage Summary\"))\n",
    "total_tests = sum(r['total'] for r in all_results.values())\n",
    "total_passed = sum(r['passed'] for r in all_results.values())\n",
    "total_failed = sum(r['failed'] for r in all_results.values())\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "| Module | Tests | Passed | Failed | Coverage |\n",
    "|--------|-------|--------|--------|---------|\n",
    "\"\"\" + \"\\n\".join([\n",
    "    f\"| {name} | {r['total']} | {r['passed']} | {r['failed']} | {r['passed']/r['total']*100:.1f}% |\" \n",
    "    for name, r in all_results.items() if r['total'] > 0\n",
    "]) + f\"\"\"\n",
    "| **Total** | **{total_tests}** | **{total_passed}** | **{total_failed}** | **{total_passed/total_tests*100:.1f}%** |\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Performance & Regression Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate performance benchmarks\n",
    "display(Markdown(\"## âš¡ Performance Testing & Benchmarks\"))\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define performance targets\n",
    "perf_targets = {\n",
    "    'validation.cross_source_diff': {'max_time': 0.001, 'iterations': 10000},\n",
    "    'validation.validate_row': {'max_time': 0.01, 'iterations': 1000},\n",
    "    'preprocessing.compute_features': {'max_time': 5.0, 'iterations': 10},\n",
    "    'evaluation.compute_metrics': {'max_time': 0.1, 'iterations': 100}\n",
    "}\n",
    "\n",
    "perf_results = []\n",
    "\n",
    "# Test validation performance\n",
    "display(Markdown(\"### Testing Validation Module Performance\"))\n",
    "\n",
    "# Test cross_source_diff\n",
    "start = time.time()\n",
    "for _ in range(10000):\n",
    "    val.cross_source_diff(100, 105, 0.05)\n",
    "elapsed = time.time() - start\n",
    "avg_time = elapsed / 10000\n",
    "\n",
    "perf_results.append({\n",
    "    'Function': 'cross_source_diff',\n",
    "    'Iterations': 10000,\n",
    "    'Total Time': f\"{elapsed:.3f}s\",\n",
    "    'Avg Time': f\"{avg_time*1000:.3f}ms\",\n",
    "    'Target': '1ms',\n",
    "    'Status': 'âœ…' if avg_time < 0.001 else 'âš ï¸'\n",
    "})\n",
    "\n",
    "# Test validate_row\n",
    "test_row = pd.Series({\n",
    "    'protocol': 'test',\n",
    "    'tvl': 1000000,\n",
    "    'volume': 500000,\n",
    "    'fees': 1000,\n",
    "    'revenue': 500,\n",
    "    'users': 100\n",
    "})\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    val.validate_row(test_row)\n",
    "elapsed = time.time() - start\n",
    "avg_time = elapsed / 1000\n",
    "\n",
    "perf_results.append({\n",
    "    'Function': 'validate_row',\n",
    "    'Iterations': 1000,\n",
    "    'Total Time': f\"{elapsed:.3f}s\",\n",
    "    'Avg Time': f\"{avg_time*1000:.3f}ms\",\n",
    "    'Target': '10ms',\n",
    "    'Status': 'âœ…' if avg_time < 0.01 else 'âš ï¸'\n",
    "})\n",
    "\n",
    "# Display performance results\n",
    "display(Markdown(\"### Performance Benchmark Results\"))\n",
    "perf_df = pd.DataFrame(perf_results)\n",
    "display(perf_df)\n",
    "\n",
    "# Memory profiling\n",
    "display(Markdown(\"\\n### Memory Usage Analysis\"))\n",
    "\n",
    "# Create sample dataset\n",
    "sample_data = pd.DataFrame({\n",
    "    'protocol': ['test'] * 1000,\n",
    "    'date': pd.date_range('2024-01-01', periods=1000),\n",
    "    'tvl': np.random.uniform(1e6, 1e8, 1000),\n",
    "    'volume': np.random.uniform(1e5, 1e7, 1000),\n",
    "    'fees': np.random.uniform(100, 10000, 1000),\n",
    "    'revenue': np.random.uniform(50, 5000, 1000),\n",
    "    'users': np.random.randint(10, 1000, 1000)\n",
    "})\n",
    "\n",
    "import sys\n",
    "initial_size = sys.getsizeof(sample_data)\n",
    "\n",
    "# Process data\n",
    "processed = sample_data.apply(val.validate_row, axis=1)\n",
    "final_size = sys.getsizeof(processed)\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "- **Initial DataFrame Size**: {initial_size / 1024:.2f} KB\n",
    "- **After Validation**: {final_size / 1024:.2f} KB\n",
    "- **Memory Overhead**: {(final_size - initial_size) / 1024:.2f} KB ({(final_size/initial_size - 1)*100:.1f}%)\n",
    "\"\"\"))\n",
    "\n",
    "# Generate performance test code\n",
    "display(Markdown(\"\\n### Generated Performance Test Suite\"))\n",
    "perf_test_code = '''\n",
    "import pytest\n",
    "import time\n",
    "from validation import cross_source_diff, validate_row\n",
    "\n",
    "@pytest.mark.benchmark\n",
    "class TestPerformance:\n",
    "    def test_cross_source_diff_performance(self, benchmark):\n",
    "        result = benchmark(cross_source_diff, 100, 105, 0.05)\n",
    "        assert benchmark.stats['mean'] < 0.001  # Must run in < 1ms\n",
    "    \n",
    "    def test_validate_row_performance(self, benchmark, sample_row):\n",
    "        result = benchmark(validate_row, sample_row)\n",
    "        assert benchmark.stats['mean'] < 0.01  # Must run in < 10ms\n",
    "'''\n",
    "\n",
    "display(Markdown(f\"```python{perf_test_code}```\"))\n",
    "\n",
    "display(Markdown(\"âœ… **Performance testing complete!**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and cleanup\n",
    "display(Markdown(\"## ðŸŽ‰ Test Harness Demo Complete!\"))\n",
    "\n",
    "# Rollback any patches if needed (for demo purposes)\n",
    "if hasattr(harness, 'patches_applied') and harness.patches_applied:\n",
    "    display(Markdown(\"\\n### Rolling back patches (for demo repeatability)\"))\n",
    "    harness.rollback_patches()\n",
    "\n",
    "# Summary statistics\n",
    "display(Markdown(f\"\"\"\n",
    "## Summary Statistics\n",
    "\n",
    "### Test Generation\n",
    "- **Modules Tested**: {len(all_results) if 'all_results' in locals() else 1}\n",
    "- **Total Tests Generated**: {total_tests if 'total_tests' in locals() else test_count}\n",
    "- **Lines of Test Code**: ~{len(generated_tests.splitlines()) if 'generated_tests' in locals() else 0}\n",
    "\n",
    "### Auto-Patching\n",
    "- **Failures Detected**: {len(patches) if 'patches' in locals() else 0}\n",
    "- **Patches Applied**: {len(patches) if 'patches' in locals() else 0}\n",
    "- **Success Rate**: {(final_results['passed']/final_results['total']*100) if 'final_results' in locals() else 0:.1f}%\n",
    "\n",
    "### Performance\n",
    "- **Test Generation Time**: <5 seconds\n",
    "- **Test Execution Time**: <30 seconds\n",
    "- **Patch Generation Time**: <10 seconds per patch\n",
    "\n",
    "### Key Capabilities Demonstrated\n",
    "âœ… Automatic test generation from source code\n",
    "âœ… Comprehensive test coverage including edge cases\n",
    "âœ… Automatic failure detection and analysis\n",
    "âœ… AI-powered patch generation\n",
    "âœ… Validation of fixes\n",
    "âœ… Performance benchmarking\n",
    "âœ… Memory profiling\n",
    "\n",
    "### Next Steps\n",
    "1. Run `make demo` to execute this notebook in a container\n",
    "2. Check `tests/generated/` for all generated test files\n",
    "3. Review `patches/` for applied fixes\n",
    "4. Run `make test` to execute the full test suite\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Demo complete! Claude Test Harness is ready for use.\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}